{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4981c783-f034-45ca-9a07-a9cc57f85c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "import cv2\n",
    "from mrcnn.visualize import display_instances\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b82315c-09ce-4a76-a7ca-7742e31696d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Keras version: 3.8.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b762d36-8854-4182-8caf-c5c4305a02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ROOT_DIR to the current working directory\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58e5346c-da15-4458-9c40-b35d2de29eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Mask RCNN # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16da1f14-6ccc-432a-a9f7-91842d5a06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to trained weights file\n",
    "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "faaddd32-f681-4f93-8475-02328076e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save logs and model checkpoints, if not provided\n",
    "# through the command line argument --logs\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c4e812a-f072-4bac-a1f3-e9a89f494421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConfig(Config):\n",
    "    \"\"\"Configuration for training on the custom  dataset.\n",
    "    Derives from the base Config class and overrides some values.\n",
    "    \"\"\"\n",
    "    NAME = \"object\"  # Give the configuration a recognizable name\n",
    "    GPU_COUNT = 1    # NUMBER OF GPUs to use. When using only a CPU, this needs to be set to 1.\n",
    "    IMAGES_PER_GPU = 4\n",
    "    \n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # Background + fracture\n",
    "\n",
    "    STEPS_PER_EPOCH = 5 # Number of training steps per epoch\n",
    "\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9   # Skip detections with < 90% confidence\n",
    "    LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a78ab-8238-446d-847c-d435363a6d83",
   "metadata": {},
   "source": [
    "Custom Data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08be6228-7d26-4eb3-abd4-d83525fffafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(utils.Dataset):\n",
    "\n",
    "    def load_custom(self, dataset_dir, subset):\n",
    "        \"\"\"Load a subset of the Dog-Cat dataset.\n",
    "        dataset_dir: Root directory of the dataset.\n",
    "        subset: Subset to load: train or val\n",
    "        \"\"\"\n",
    "      \n",
    "        # Add classes. We have only one class to add.\n",
    "        self.add_class(\"object\", 1, \"fracture\")\n",
    "\n",
    "        # Train or validation dataset?\n",
    "        assert subset in [\"train\", \"val\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "\n",
    "        # We mostly care about the x and y coordinates of each region\n",
    "        annotations1 = json.load(open('C:/Users/joshu/Documents/computervision/CVI03.HS24-Computer-Vision/dataset/train/train.json'))\n",
    "        # print(annotations1)\n",
    "        annotations = list(annotations1.values())  # don't need the dict keys\n",
    "\n",
    "        # The VIA tool saves images in the JSON even if they don't have any\n",
    "        # annotations. Skip unannotated images.\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "        \n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "            # print(a)\n",
    "            # Get the x, y coordinaets of points of the polygons that make up\n",
    "            # the outline of each object instance. There are stores in the\n",
    "            # shape_attributes (see json format above)\n",
    "            polygons = [r['shape_attributes'] for r in a['regions']] \n",
    "            objects = [s['region_attributes']['names'] for s in a['regions']]\n",
    "            print(\"objects:\",objects)\n",
    "            name_dict = {\"fracture\": 1}\n",
    "\n",
    "            # key = tuple(name_dict)\n",
    "            num_ids = [name_dict[a] for a in objects]\n",
    "     \n",
    "            # num_ids = [int(n['Event']) for n in objects]\n",
    "            # load_mask() needs the image size to convert polygons to masks.\n",
    "            # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
    "            # the image. This is only managable since the dataset is tiny.\n",
    "            print(\"numids\",num_ids)\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            self.add_image(\n",
    "                \"object\",  ## for a single class just add the name here\n",
    "                image_id=a['filename'],  # use file name as a unique image id\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                polygons=polygons,\n",
    "                num_ids=num_ids\n",
    "                )\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a Dog-Cat dataset image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"object\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "        # Convert polygons to a bitmap mask of shape\n",
    "        # [height, width, instance_count]\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] != \"object\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "        num_ids = info['num_ids']\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\n",
    "        \trr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "\n",
    "        \tmask[rr, cc, i] = 1\n",
    "\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID only, we return an array of 1s\n",
    "        # Map class names to class IDs.\n",
    "        num_ids = np.array(num_ids, dtype=np.int32)\n",
    "        return mask, num_ids #np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"object\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c89ef4-6808-4a82-9fad-861e0d0e64a8",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fea5be3-a9f2-4313-936d-447d73c5bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Training dataset.\n",
    "    dataset_train = CustomDataset()\n",
    "    dataset_train.load_custom(\"C:/Users/joshu/Documents/computervision/CVI03.HS24-Computer-Vision/dataset\", \"train\")\n",
    "    dataset_train.prepare()\n",
    "\n",
    "    # Validation dataset\n",
    "    dataset_val = CustomDataset()\n",
    "    dataset_val.load_custom(\"C:/Users/joshu/Documents/computervision/CVI03.HS24-Computer-Vision/dataset\", \"val\")\n",
    "    dataset_val.prepare()\n",
    "\n",
    "    # *** This training schedule is an example. Update to your needs ***\n",
    "    # Since we're using a very small dataset, and starting from\n",
    "    # COCO trained weights, we don't need to train too long. Also,\n",
    "    # no need to train all layers, just the heads should do it.\n",
    "    \n",
    "    # print(\"Training network heads\")\n",
    "    \n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=250,\n",
    "                layers='heads')\n",
    "                \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cf61d81-cf2d-4144-b1a5-8cc32d6f4a2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, None, 4), dtype=float32, sparse=False, name=input_gt_boxes>',)\n  • kwargs={'mask': 'None'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m CustomConfig()\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodellib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaskRCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_LOGS_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m COCO_WEIGHTS_PATH\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# Download weights file\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\computervision\\CVI03.HS24-Computer-Vision\\mrcnn\\model.py:1842\u001b[0m, in \u001b[0;36mMaskRCNN.__init__\u001b[1;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir \u001b[38;5;241m=\u001b[39m model_dir\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_log_dir()\n\u001b[1;32m-> 1842\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeras_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\computervision\\CVI03.HS24-Computer-Vision\\mrcnn\\model.py:1880\u001b[0m, in \u001b[0;36mMaskRCNN.build\u001b[1;34m(self, mode, config)\u001b[0m\n\u001b[0;32m   1877\u001b[0m input_gt_boxes \u001b[38;5;241m=\u001b[39m KL\u001b[38;5;241m.\u001b[39mInput(\n\u001b[0;32m   1878\u001b[0m     shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m4\u001b[39m], name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_gt_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m   1879\u001b[0m \u001b[38;5;66;03m# Normalize coordinates\u001b[39;00m\n\u001b[1;32m-> 1880\u001b[0m gt_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mKL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_boxes_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_gt_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[38;5;66;03m# 3. GT Masks (zero padded)\u001b[39;00m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;66;03m# [batch, height, width, MAX_GT_INSTANCES]\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mUSE_MINI_MASK:\n",
      "File \u001b[1;32m~\\Documents\\computervision\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Documents\\computervision\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\lambda_layer.py:95\u001b[0m, in \u001b[0;36mLambda.compute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mshape, output_spec)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m     96\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe could not automatically infer the shape of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe Lambda\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms output. Please specify the `output_shape` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument for this Lambda layer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m         )\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape(input_shape)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, None, 4), dtype=float32, sparse=False, name=input_gt_boxes>',)\n  • kwargs={'mask': 'None'}"
     ]
    }
   ],
   "source": [
    "config = CustomConfig()\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                                  model_dir=DEFAULT_LOGS_DIR)\n",
    "\n",
    "weights_path = COCO_WEIGHTS_PATH\n",
    "        # Download weights file\n",
    "if not os.path.exists(weights_path):\n",
    "  utils.download_trained_weights(weights_path)\n",
    "\n",
    "model.load_weights(weights_path, by_name=True, exclude=[\n",
    "            \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
    "            \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "train(model)\t\t\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
